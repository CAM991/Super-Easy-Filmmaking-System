# ======================
# Super-Easy Filmmaking System
# ======================

# Step 1: æ¸…ç†ç¯å¢ƒ
!pip uninstall -y datasets fsspec gcsfs torchtune

# Step 2: æ‰‹åŠ¨å®‰è£…ç¼ºå¤±çš„ä¾èµ–
# ======================
!pip install datasets==2.14.6 fsspec==2023.10.0

!pip install --no-deps -q gradio diffusers transformers accelerate torch torchvision torchaudio gTTS safetensors moviepy opencv-python addict
# Step 3: è¿è¡Œå®Œæ•´ SEFS ä»£ç 

# å¯¼å…¥åº“
import gradio as gr
from diffusers import StableDiffusionPipeline, AnimateDiffPipeline, DDIMScheduler
from diffusers.models import UNet2DConditionModel
from diffusers.utils import export_to_gif
import torch
from gtts import gTTS
import os

# ======================
# GPU éªŒè¯
# ======================
print("CUDA available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("Using GPU:", torch.cuda.get_device_name(0))

# ======================
# 1. æ–‡å­—ç”Ÿå›¾ (Text2Image)
# ======================
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
    use_safetensors=True
).to("cuda")
pipe.enable_attention_slicing()

def text_to_image(prompt):
    with torch.autocast("cuda"):
        image = pipe(prompt).images[0]
    return image

# ======================
# 2. è¯­éŸ³åˆæˆ (text2audio)
# ======================
def text_to_speech(text):
    lang = "zh" if any('\u4e00' <= char <= '\u9fff' for char in text) else "en"
    tts = gTTS(text=text, lang=lang, slow=False)
    tts.save("output.mp3")
    return "output.mp3"

# ======================
# ======================
# 3. ç”ŸåŠ¨å›¾ (image2video)
# ======================
from diffusers import MotionAdapter, AnimateDiffPipeline, DDIMScheduler
from diffusers.utils import export_to_gif
import torch

# åˆå§‹åŒ–ä¸€æ¬¡ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
_motion_pipe = None

def get_animate_pipe():
    global _motion_pipe
    if _motion_pipe is None:
        adapter = MotionAdapter.from_pretrained(
            "guoyww/animatediff-motion-adapter-v1-5-2",
            torch_dtype=torch.float16
        )
        pipe = AnimateDiffPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            motion_adapter=adapter,
            torch_dtype=torch.float16
        ).to("cuda")
        pipe.scheduler = DDIMScheduler.from_config(
            pipe.scheduler.config,
            timestep_spacing="trailing",
            beta_schedule="linear"
        )
        _motion_pipe = pipe
    return _motion_pipe

def image_to_video(image_path, prompt=""):
    try:
        pipe = get_animate_pipe()
        result = pipe(
            prompt=prompt if prompt else "A beautiful animated scene",
            num_frames=16,
            guidance_scale=7.5,
            num_inference_steps=25
        )
        output_path = "output_animation.gif"
        export_to_gif(result.frames[0], output_path)
        return output_path
    except Exception as e:
        print(f"ç”Ÿæˆå¤±è´¥: {str(e)}")
        return None

# ======================
# 4. è§†é¢‘å‰ªè¾‘
# ======================
video_clips = []

def add_clip(video_path):
    global video_clips
    if video_path and os.path.exists(video_path):
        video_clips.append(video_path)
    return f"æ—¶é—´çº¿: {len(video_clips)} ä¸ªç‰‡æ®µ"

def export_final_video():
    global video_clips
    if not video_clips:
        return None
    
    try:
        clips = [VideoFileClip(clip) for clip in video_clips]
        final_clip = concatenate_videoclips(clips, method="compose")
        output_path = "final_movie.mp4"
        final_clip.write_videofile(output_path, fps=8, verbose=False, logger=None)
        
        for clip in clips:
            clip.close()
        final_clip.close()
        return output_path
    except Exception as e:
        return None

def reset_timeline():
    global video_clips
    video_clips = []
    return "æ—¶é—´çº¿å·²æ¸…ç©º"

# ======================
# åˆ›å»ºç•Œé¢
# ======================
with gr.Blocks(title="Super-Easy Filmmaking System") as demo:
    gr.Markdown("# ğŸ¬ Super-Easy Filmmaking System")
    gr.Markdown("### é›¶æˆæœ¬AIç”µå½±åˆ¶ä½œç³»ç»Ÿ | æ‰€æœ‰åŠŸèƒ½å…è´¹ä½¿ç”¨")
    
    with gr.Tab("1. æ–‡å­—ç”Ÿå›¾ Text2Image"):
        txt = gr.Textbox(label="åœºæ™¯æè¿°ï¼ˆè‹±æ–‡æ•ˆæœæ›´ä½³ï¼‰", placeholder="e.g., A cyberpunk city at night, raining")
        btn1 = gr.Button("ç”Ÿæˆå›¾ç‰‡ï¼ˆGPUåŠ é€Ÿï¼‰")
        img_out = gr.Image()
        btn1.click(text_to_image, inputs=txt, outputs=img_out)
    
    with gr.Tab("2. è¯­éŸ³åˆæˆ Text2Audio"):
        txt2 = gr.Textbox(label="å°è¯ï¼ˆè‡ªåŠ¨è¯†åˆ«ä¸­/è‹±æ–‡ï¼‰", placeholder="è¾“å…¥æ‚¨çš„å°è¯...")
        btn2 = gr.Button("ç”Ÿæˆè¯­éŸ³")
        audio_out = gr.Audio()
        btn2.click(text_to_speech, inputs=txt2, outputs=audio_out)
    
    with gr.Tab("3. å›¾ç‰‡ç”ŸåŠ¨å›¾ Image2Video"):
        with gr.Row():
            img_in = gr.Image(type="filepath", label="ä¸Šä¼ å›¾ç‰‡")
            prompt_in = gr.Textbox(label="åŠ¨ç”»æè¿°ï¼ˆå¯é€‰ï¼‰", placeholder="e.g., camera slowly zooms in")
        btn3 = gr.Button("ç”ŸæˆåŠ¨ç”»ï¼ˆçº¦30ç§’ï¼‰")
        video_out = gr.Video()
        btn3.click(image_to_video, inputs=[img_in, prompt_in], outputs=video_out)
    
    with gr.Tab("4. è§†é¢‘å‰ªè¾‘ Video-Edit"):
        gr.Markdown("### ğŸï¸ å°†ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µæ‹¼æ¥æˆå®Œæ•´ç”µå½±")
        with gr.Row():
            clip_input = gr.Video(label="é€‰æ‹©è§†é¢‘ç‰‡æ®µ")
            add_btn = gr.Button("æ·»åŠ åˆ°æ—¶é—´çº¿")
        status = gr.Textbox(label="çŠ¶æ€")
        with gr.Row():
            export_btn = gr.Button("å¯¼å‡ºæœ€ç»ˆç”µå½±", variant="primary")
            reset_btn = gr.Button("æ¸…ç©ºæ—¶é—´çº¿")
        final_video = gr.Video(label="æ‚¨çš„ç”µå½±ä½œå“")
        
        add_btn.click(add_clip, inputs=clip_input, outputs=status)
        export_btn.click(export_final_video, outputs=final_video)
        reset_btn.click(reset_timeline, outputs=status)

# å¯åŠ¨
demo.launch(share=True)
